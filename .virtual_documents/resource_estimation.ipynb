import numpy as np
from scipy.special import binom


def openai_flops_per_token(n_layers, n_heads, d_model, n_ctx, n_vocab, ff_ratio=4):
    """Open AI method for forward pass FLOPs counting of decoder-only Transformer"""
    d_attn = d_model // n_heads
    d_ff = d_model * ff_ratio
 
    embeddings = 4 * d_model
    attn_qkv = 2 * n_layers * d_model * 3 * (d_attn * n_heads)
    attn_mask = 2 * n_layers * n_ctx * (d_attn * n_heads)
    attn_project = 2 * n_layers * (d_attn * n_heads) * d_model
    ff = 2 * n_layers * 2 * d_model * d_ff
    logits = 2 * d_model * n_vocab
 
    return embeddings + attn_qkv + attn_mask + attn_project + ff + logits

def deepmind_flops_per_sequence(n_layers, n_heads, d_model, n_ctx, n_vocab, ff_ratio=4):
    """DeepMind method for forward pass FLOPs counting of decoder-only Transformer"""
    d_attn = d_model // n_heads
    d_ff = d_model * ff_ratio
 
    embeddings = 2 * n_ctx * n_vocab * d_model
 
    attn_qkv = 2 * n_ctx * 3 * d_model * (d_attn * n_heads)
    attn_logits = 2 * n_ctx * n_ctx * (d_attn * n_heads)
    attn_softmax = 3 * n_heads * n_ctx * n_ctx
    attn_reduce = 2 * n_ctx * n_ctx * (d_attn * n_heads)
    attn_project = 2 * n_ctx * (d_attn * n_heads) * d_model
    total_attn = attn_qkv + attn_logits + attn_softmax + attn_reduce + attn_project
 
    ff = 2 * n_ctx * (d_model * d_ff + d_model * d_ff)
 
    logits = 2 * n_ctx * d_model * n_vocab
 
    return embeddings + n_layers * (total_attn + ff) + logits


# n_layers = 12; d_model = 768; n_heads = 12; n_vocab = 50257; n_ctx = 4096
n_layers = 24; d_model = 1024; n_heads = 16; n_vocab = 50257; n_ctx = 4096

openai_fpt = openai_flops_per_token(n_layers, n_heads, d_model, n_ctx, n_vocab, ff_ratio=4)
deepmind_fpt = openai_flops_per_token(n_layers, n_heads, d_model, n_ctx, n_vocab, ff_ratio=4) 


print(openai_fpt)
print(deepmind_fpt)


def hrnn_operations_per_sequence(k, n_ctx):
    """kHRNN forward pass FLOPs counting of decoder-only Transformer"""
    mcZ_per_cell = [binom(k, i) for i in range(1,k)]
    cX_per_cell = k
    mcZ_per_cell[-1] += 1
    
    mcZ_total = n_ctx * mcZ_per_cell
    cX_total = n_ctx * cX_per_cell

    return mcZ_total, cX_total

def hrnn_training_flops(k, d_model):
    """FLOPs required to train a k-HRNN is equivalent to training a classical RNN of model size d_model = (n choose k) where n is the number of qubits in the k-HRNN"""
    return


n_mcZ, n_cX = hrnn_operations_per_token(5, d_model, n_ctx)
print(n_mcZ, n_cX)



